# Airavata Scheduler

A production-ready distributed task execution system for scientific computing experiments with a crystal-clear hexagonal architecture, cost-based scheduling, and comprehensive data management.

## ğŸ¯ Conceptual Model

The Airavata Scheduler is built around **6 core domain interfaces** that represent the fundamental operations of distributed task execution, with a **gRPC-based worker system** for task execution:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Core Domain Interfaces                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ResourceRegistry    â”‚  CredentialVault     â”‚  ExperimentOrch   â”‚
â”‚  â€¢ Register compute  â”‚  â€¢ Secure storage    â”‚  â€¢ Create exper   â”‚
â”‚  â€¢ Register storage  â”‚  â€¢ Unix permissions  â”‚  â€¢ Generate tasks â”‚
â”‚  â€¢ Validate access   â”‚  â€¢ Encrypt/decrypt   â”‚  â€¢ Submit for execâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TaskScheduler       â”‚  DataMover          â”‚  WorkerLifecycle  â”‚
â”‚  â€¢ Cost optimization â”‚  â€¢ 3-hop staging    â”‚  â€¢ Spawn workers  â”‚
â”‚  â€¢ Worker distrib    â”‚  â€¢ Persistent cache â”‚  â€¢ gRPC workers   â”‚
â”‚  â€¢ Atomic assignment â”‚  â€¢ Lineage tracking â”‚  â€¢ Task execution â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    gRPC Worker System                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Worker Binary      â”‚  Script Generation   â”‚  Task Execution   â”‚
â”‚  â€¢ Standalone exec  â”‚  â€¢ SLURM scripts     â”‚  â€¢ Poll for tasks â”‚
â”‚  â€¢ gRPC client      â”‚  â€¢ K8s manifests     â”‚  â€¢ Execute tasks  â”‚
â”‚  â€¢ Auto-deployment  â”‚  â€¢ Bare metal scriptsâ”‚  â€¢ Report results â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ—ï¸ Hexagonal Architecture

The system implements a **clean hexagonal architecture** (ports-and-adapters) with a `core/` directory structure:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Airavata Scheduler                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core Domain Layer (Business Logic)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚domain/      â”‚ â”‚domain/      â”‚ â”‚domain/      â”‚              â”‚
â”‚  â”‚models.go    â”‚ â”‚interface.go â”‚ â”‚enum.go      â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚domain/      â”‚ â”‚domain/      â”‚ â”‚domain/      â”‚              â”‚
â”‚  â”‚value.go     â”‚ â”‚error.go     â”‚ â”‚event.go     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core Services Layer (Implementation)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚service/     â”‚ â”‚service/     â”‚ â”‚service/     â”‚              â”‚
â”‚  â”‚registry.go  â”‚ â”‚vault.go     â”‚ â”‚orchestrator â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚service/     â”‚ â”‚service/     â”‚ â”‚service/     â”‚              â”‚
â”‚  â”‚scheduler.go â”‚ â”‚datamover.go â”‚ â”‚worker.go    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core Ports Layer (Infrastructure Interfaces)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚port/        â”‚ â”‚port/        â”‚ â”‚port/        â”‚              â”‚
â”‚  â”‚database.go  â”‚ â”‚cache.go     â”‚ â”‚events.go    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚port/        â”‚ â”‚port/        â”‚ â”‚port/        â”‚              â”‚
â”‚  â”‚security.go  â”‚ â”‚storage.go   â”‚ â”‚compute.go   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Adapters Layer (External Integrations)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚HTTP         â”‚ â”‚PostgreSQL   â”‚ â”‚SLURM/K8s    â”‚              â”‚
â”‚  â”‚WebSocket    â”‚ â”‚Redis        â”‚ â”‚S3/NFS/SFTP  â”‚              â”‚
â”‚  â”‚gRPC Worker  â”‚ â”‚Cache        â”‚ â”‚Bare Metal   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ› ï¸ Technology Stack

- **Language**: Go 1.21+
- **Database**: PostgreSQL 15+ 
- **ORM**: GORM v2 with PostgreSQL driver
- **Storage Adapters**: SFTP, S3, NFS
- **Compute Adapters**: SLURM, Kubernetes, Bare Metal
- **Authentication**: JWT with bcrypt password hashing
- **Credential Storage**: OpenBao with AES-256-GCM encryption
- **Authorization**: SpiceDB with Zanzibar model for fine-grained permissions
- **Data Caching**: PostgreSQL-backed with lineage tracking
- **API Framework**: Gorilla Mux for HTTP routing
- **gRPC**: Protocol Buffers for worker communication
- **Architecture**: Hexagonal (ports-and-adapters) pattern

**Production Ready - Clean hexagonal architecture**

## ğŸš€ Quick Start

### Cold Start (Recommended for Testing)

For a complete cold start from scratch (no existing containers or volumes):

```bash
# Complete cold start setup - builds everything from scratch
./scripts/setup-cold-start.sh

# This script automatically:
# 1. Validates prerequisites (Go, Docker, ports)
# 2. Downloads Go dependencies
# 3. Generates protobuf files
# 4. Creates deterministic SLURM munge key
# 5. Starts all services with test profile
# 6. Waits for services to be healthy
# 7. Uploads SpiceDB schema
# 8. Builds all binaries
```

### Integration Tests

Run comprehensive integration tests across all compute and storage types:

```bash
# Run integration tests (includes cold start)
./scripts/test/run-integration-tests.sh

# This validates:
# - SLURM clusters (both cluster 1 and cluster 2)
# - Bare metal compute nodes
# - Storage backends (S3/MinIO, SFTP, NFS)
# - Credential management via SpiceDB/OpenBao
# - Workflow execution and task dependencies
# - Worker system and scheduler recovery
# - Multi-runtime experiments
```

### Manual Setup

Get up and running quickly with the Airavata Scheduler:

```bash
# 1. Build all binaries (scheduler, worker, CLI)
make build

# 2. Start services (PostgreSQL, SpiceDB, OpenBao)
# Production mode (default)
docker compose up -d postgres spicedb spicedb-postgres openbao

# Or explicitly use production profile
docker compose --profile prod up -d postgres spicedb spicedb-postgres openbao

# 3. Upload SpiceDB schema
make spicedb-schema-upload

# 4. Run your first experiment
./build/airavata-scheduler run tests/sample_experiment.yml \
  --project my-project \
  --compute cluster-1 \
  --storage s3-bucket-1 \
  --watch
```

For detailed setup instructions, see the [Quick Start Guide](docs/guides/quickstart.md).

## ğŸ–¥ï¸ Command Line Interface

The Airavata Scheduler includes a comprehensive CLI (`airavata`) for complete system management:

### Complete Workflow Example

```bash
# 1. Authenticate
./bin/airavata auth login

# 2. Create project
./bin/airavata project create

# 3. Upload input data
./bin/airavata data upload input.dat minio-storage:/experiments/input.dat

# 4. Run experiment
./bin/airavata experiment run experiment.yml --project proj-123 --compute slurm-1

# 5. Monitor experiment
./bin/airavata experiment watch exp-456

# 6. Check outputs
./bin/airavata experiment outputs exp-456

# 7. Download results
./bin/airavata experiment download exp-456 --output ./results/
```

### Key CLI Features

- **Data Management**: Upload/download files and directories to/from any storage type
- **Experiment Lifecycle**: Run, monitor, cancel, pause, resume, and retry experiments
- **Output Collection**: Download experiment outputs organized by task with archive support
- **Project Management**: Create projects, manage team members, and organize experiments
- **Resource Management**: Register compute/storage resources with credential binding and verification
- **Real-time Monitoring**: Watch experiments with live status updates and logs

### CLI Command Groups

```bash
# Authentication and configuration
airavata auth login|logout|status
airavata config set|get|show

# User and project management
airavata user profile|update|password|groups|projects
airavata project create|list|get|update|delete|members|add-member|remove-member

# Resource management
airavata resource compute list|get|create|update|delete
airavata resource storage list|get|create|update|delete
airavata resource credential list|create|delete
airavata resource bind-credential|unbind-credential|test-credential
airavata resource status|metrics|test

# Data management
airavata data upload|upload-dir|download|download-dir|list

# Experiment management
airavata experiment run|status|watch|list|outputs|download
airavata experiment cancel|pause|resume|logs|resubmit|retry
airavata experiment tasks|task
```

For complete CLI documentation, see [CLI Reference](docs/reference/cli.md).

### Cold-Start Testing (Fresh Clone)

For testing on a fresh clone with only Docker and Go:

```bash
# Clone and enter directory
git clone <repo-url>
cd airavata-scheduler

# Run cold-start setup
chmod +x scripts/*.sh
./scripts/setup-cold-start.sh

# Run all tests
make cold-start-test

# Or run individual test suites
make test-unit
make test-integration
```

**Prerequisites**: Docker and Go 1.21+ must be in PATH.

## ğŸ” Credential Management Architecture

The Airavata Scheduler implements a **three-layer credential architecture** that separates authorization logic from storage for maximum security and scalability:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                        â”‚
â”‚   (Experiments, Resources, Users, Groups)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                  â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL     â”‚  â”‚    SpiceDB      â”‚  â”‚   OpenBao      â”‚
â”‚                  â”‚  â”‚                 â”‚  â”‚                â”‚
â”‚  Domain Data     â”‚  â”‚  Authorization  â”‚  â”‚  Secrets       â”‚
â”‚  - Users         â”‚  â”‚  - Permissions  â”‚  â”‚  - SSH Keys    â”‚
â”‚  - Groups        â”‚  â”‚  - Ownership    â”‚  â”‚  - Passwords   â”‚
â”‚  - Experiments   â”‚  â”‚  - Sharing      â”‚  â”‚  - Tokens      â”‚
â”‚  - Resources     â”‚  â”‚  - Hierarchies  â”‚  â”‚  (Encrypted)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Benefits

- **ğŸ”’ Separation of Concerns**: Authorization (SpiceDB) separate from secret storage (OpenBao)
- **ğŸ›¡ï¸ Fine-grained Permissions**: Read/write/delete permissions with hierarchical group inheritance
- **ğŸ“‹ Complete Audit Trail**: All operations logged across all three systems
- **ğŸ”„ Credential Rotation**: Support for automatic key rotation with zero downtime
- **ğŸ‘¥ Group Management**: Groups can contain groups with transitive permission inheritance
- **ğŸ”— Resource Binding**: Credentials bound to specific compute/storage resources

### Quick Start

```bash
# 1. Start all services including SpiceDB and OpenBao
make docker-up
make wait-services
make spicedb-schema-upload

# 2. Verify services
curl -s http://localhost:8200/v1/sys/health | jq  # OpenBao
curl -s http://localhost:50052/healthz            # SpiceDB

# 3. Create and share credentials via API
curl -X POST http://localhost:8080/api/v1/credentials \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"name": "cluster-ssh", "type": "ssh_key", "data": "..."}'

# 4. Share with group
curl -X POST http://localhost:8080/api/v1/credentials/cred-123/share \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"principal_type": "group", "principal_id": "team-1", "permission": "read"}'

# 5. Bind to resource
curl -X POST http://localhost:8080/api/v1/credentials/cred-123/bind \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"resource_type": "compute", "resource_id": "cluster-1"}'
```

### Credential Resolution Flow

When an experiment runs, the system automatically:

1. **Identifies Required Resources**: Determines compute and storage resources needed
2. **Finds Bound Credentials**: Queries SpiceDB for credentials bound to each resource
3. **Checks User Permissions**: Verifies user has read access to each credential
4. **Retrieves Secrets**: Decrypts credential data from OpenBao
5. **Uses for Execution**: Provides credentials to workers for resource access

### Documentation

- **[Quick Start Guide](docs/guides/quickstart.md)** - Get up and running quickly
- **[Credential Management](docs/guides/credential-management.md)** - Complete credential system guide
- **[Deployment Guide](docs/guides/deployment.md)** - Production deployment instructions
- **[API Reference](docs/reference/api.md)** - Complete API documentation
- **[Architecture Overview](docs/reference/architecture.md)** - System design and patterns

## ğŸ“ Project Structure

```
airavata-scheduler/
â”œâ”€â”€ core/                     # Core application code
â”‚   â”œâ”€â”€ domain/              # Business logic and entities
â”‚   â”‚   â”œâ”€â”€ interface.go     # 6 core domain interfaces
â”‚   â”‚   â”œâ”€â”€ model.go         # Domain entities
â”‚   â”‚   â”œâ”€â”€ enum.go          # Status enums and types
â”‚   â”‚   â”œâ”€â”€ value.go         # Value objects
â”‚   â”‚   â”œâ”€â”€ error.go         # Domain-specific errors
â”‚   â”‚   â””â”€â”€ event.go         # Domain events
â”‚   â”œâ”€â”€ service/             # Service implementations
â”‚   â”‚   â”œâ”€â”€ registry.go      # ResourceRegistry implementation
â”‚   â”‚   â”œâ”€â”€ vault.go         # CredentialVault implementation
â”‚   â”‚   â”œâ”€â”€ orchestrator.go  # ExperimentOrchestrator implementation
â”‚   â”‚   â”œâ”€â”€ scheduler.go     # TaskScheduler implementation
â”‚   â”‚   â”œâ”€â”€ datamover.go     # DataMover implementation
â”‚   â”‚   â””â”€â”€ worker.go        # WorkerLifecycle implementation
â”‚   â”œâ”€â”€ port/                # Infrastructure interfaces
â”‚   â”‚   â”œâ”€â”€ database.go      # Database operations
â”‚   â”‚   â”œâ”€â”€ cache.go         # Caching operations
â”‚   â”‚   â”œâ”€â”€ events.go        # Event publishing
â”‚   â”‚   â”œâ”€â”€ security.go      # Authentication/authorization
â”‚   â”‚   â”œâ”€â”€ storage.go       # File storage
â”‚   â”‚   â”œâ”€â”€ compute.go       # Compute resource interaction
â”‚   â”‚   â””â”€â”€ metric.go        # Metrics collection
â”‚   â”œâ”€â”€ dto/                 # Data transfer objects
â”‚   â”‚   â”œâ”€â”€ *.pb.go          # Generated protobuf types
â”‚   â”‚   â””â”€â”€ *_grpc.pb.go     # Generated gRPC service code
â”‚   â”œâ”€â”€ app/                 # Application bootstrap
â”‚   â”‚   â”œâ”€â”€ bootstrap.go     # Dependency injection and wiring
â”‚   â”‚   â””â”€â”€ factory.go       # Service factories
â”‚   â”œâ”€â”€ cmd/                 # Main application entry point
â”‚   â”‚   â””â”€â”€ main.go          # Scheduler server binary
â”‚   â””â”€â”€ util/                # Utility functions
â”‚       â”œâ”€â”€ common.go        # Common utilities
â”‚       â”œâ”€â”€ analytics.go     # Analytics utilities
â”‚       â””â”€â”€ websocket.go     # WebSocket utilities
â”œâ”€â”€ adapters/                # External system integrations
â”‚   â”œâ”€â”€ handler_http.go      # HTTP API handlers
â”‚   â”œâ”€â”€ handler_websocket.go # WebSocket handlers
â”‚   â”œâ”€â”€ handler_grpc_worker.go # gRPC worker service
â”‚   â”œâ”€â”€ database_postgres.go # PostgreSQL implementation
â”‚   â”œâ”€â”€ cache_inmemory.go    # In-memory cache
â”‚   â”œâ”€â”€ events_inmemory.go   # In-memory events
â”‚   â”œâ”€â”€ security_jwt.go      # JWT authentication
â”‚   â”œâ”€â”€ metrics_prometheus.go # Prometheus metrics
â”‚   â”œâ”€â”€ compute_slurm.go     # SLURM compute adapter
â”‚   â”œâ”€â”€ compute_kubernetes.go # Kubernetes compute adapter
â”‚   â”œâ”€â”€ compute_baremetal.go # Bare metal compute adapter
â”‚   â”œâ”€â”€ storage_s3.go        # S3 storage adapter
â”‚   â”œâ”€â”€ storage_nfs.go       # NFS storage adapter
â”‚   â”œâ”€â”€ storage_sftp.go      # SFTP storage adapter
â”‚   â”œâ”€â”€ script_config.go     # Script generation config
â”‚   â””â”€â”€ utils.go             # Adapter utilities
â”œâ”€â”€ cmd/                     # Application binaries
â”‚   â”œâ”€â”€ worker/              # Worker binary
â”‚   â”‚   â””â”€â”€ main.go          # Worker gRPC client
â”‚   â””â”€â”€ cli/                 # Command Line Interface
â”‚       â”œâ”€â”€ main.go          # Root CLI commands and experiment management
â”‚       â”œâ”€â”€ auth.go          # Authentication commands
â”‚       â”œâ”€â”€ user.go          # User profile and account management
â”‚       â”œâ”€â”€ resources.go     # Resource management (compute, storage, credentials)
â”‚       â”œâ”€â”€ data.go          # Data upload/download commands
â”‚       â”œâ”€â”€ project.go       # Project management commands
â”‚       â””â”€â”€ config.go        # Configuration management
â”œâ”€â”€ proto/                   # Protocol buffer definitions
â”‚   â”œâ”€â”€ worker.proto         # Worker gRPC service
â”‚   â”œâ”€â”€ scheduler.proto      # Scheduler gRPC service
â”‚   â””â”€â”€ *.proto              # Other proto definitions
â”œâ”€â”€ db/                      # Database schema and migrations
â”‚   â”œâ”€â”€ schema.sql           # Main database schema
â”‚   â””â”€â”€ migrations/          # Database migrations
â”œâ”€â”€ build/                   # Compiled binaries (gitignored)
â”‚   â”œâ”€â”€ scheduler            # Scheduler server binary
â”‚   â””â”€â”€ worker               # Worker binary
â”œâ”€â”€ tests/                   # Test suites
â”‚   â”œâ”€â”€ unit/                # Unit tests
â”‚   â”œâ”€â”€ integration/         # Integration tests
â”‚   â”œâ”€â”€ performance/         # Performance tests
â”‚   â””â”€â”€ testutil/            # Test utilities
â”œâ”€â”€ scripts/                 # Build and deployment scripts
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ architecture.md      # System architecture
â”‚   â”œâ”€â”€ development.md       # Development guide
â”‚   â”œâ”€â”€ deployment.md        # Deployment guide
â”‚   â”œâ”€â”€ api.md              # API documentation
â”‚   â””â”€â”€ api_openapi.yaml    # OpenAPI specification
â”œâ”€â”€ Makefile                 # Build automation
â”œâ”€â”€ docker-compose.yml       # Docker services
â””â”€â”€ go.mod                   # Go module definition
```

## ğŸ”§ Development

### Prerequisites

- Go 1.21+
- PostgreSQL 15+
- Docker (for testing)

### Setup

```bash
# Clone repository
git clone https://github.com/apache/airavata/scheduler.git
cd airavata-scheduler

# Install dependencies
go mod download

# Generate proto code
make proto

# Or manually
protoc --go_out=core/dto --go-grpc_out=core/dto \
    --go_opt=paths=source_relative \
    --go-grpc_opt=paths=source_relative \
    --proto_path=proto \
    proto/*.proto

# Build binaries
make build

# Setup database
createdb airavata_scheduler
psql airavata_scheduler < db/schema.sql

# Run scheduler server
./build/scheduler --mode=server

# Run worker (in separate terminal)
./build/worker --server-address=localhost:50051
```

### Docker Compose Profiles

The project uses a single `docker-compose.yml` file with profiles to support different environments:

```bash
# Production mode (default) - Core services only
docker compose up -d

# Test mode - Full test environment with compute services
docker compose --profile test up -d
```

**Test Profile (`test`):**
- 2 SLURM clusters with controllers and compute nodes
- 2 baremetal SSH servers for direct execution
- Kubernetes-in-Docker (kind) cluster
- Standard healthcheck intervals and timeouts
- Used for integration testing with production-like environment

### Testing

```bash
# Run all tests
make test

# Run unit tests
make test-unit

# Run integration tests
make test-integration

# Run performance tests
make test-performance

# Run tests with coverage
make test-coverage

# Run cold start test with CSV report (destroys containers/volumes)
make cold-start-test-csv
```

#### Cold Start Testing with CSV Reports

For comprehensive testing from a clean state with detailed reporting:

```bash
# Full cold start test with CSV report generation
make cold-start-test-csv

# Or run directly with options
./scripts/test/run-cold-start-with-report.sh [OPTIONS]

# Options:
#   --skip-cleanup        Skip Docker cleanup
#   --skip-cold-start     Skip cold start setup
#   --unit-only          Run only unit tests
#   --integration-only   Run only integration tests
#   --no-csv             Skip CSV report generation
```

This will:
1. **Destroy all containers and volumes** for a true cold start
2. **Recreate environment from scratch** using `scripts/setup-cold-start.sh`
3. **Run all test suites** (unit + integration) with JSON output
4. **Generate CSV report** with detailed test results in `logs/cold-start-test-results-[timestamp].csv`

**CSV Report Format:**
```
Category,Test Name,Status,Duration (s),Warnings/Notes
Unit,TestExample,PASS,0.123,
Integration,TestE2E,FAIL,45.67,Timeout waiting for service
```

**Status Types:**
- `PASS`: Test passed without issues
- `FAIL`: Test failed with errors  
- `SKIP`: Test skipped (service unavailable, etc.)
- `PASS_WITH_WARNING`: Test passed but had warnings

**Generated Files:**
- `logs/cold-start-test-results-[timestamp].csv` - Detailed test results
- `logs/unit-tests-[timestamp].json` - Unit test JSON output
- `logs/integration-tests-[timestamp].json` - Integration test JSON output
- `logs/cold-start-setup-[timestamp].log` - Cold start setup log

## ğŸ“Š Key Features

### ğŸ¯ Cost-Based Scheduling
- Multi-objective optimization (time, cost, deadline)
- Dynamic resource allocation
- Intelligent task distribution

### ğŸ”’ Enterprise Security
- **OpenBao Integration**: AES-256-GCM encryption with envelope encryption
- **SpiceDB Authorization**: Fine-grained permissions with Zanzibar model
- **Complete Audit Trail**: All credential operations logged for compliance
- **JWT-based Authentication**: Secure user authentication and session management
- **Credential Rotation**: Support for automatic key rotation and lifecycle management
- **Group Management**: Hierarchical group memberships with permission inheritance

### ğŸ“ˆ Real-Time Monitoring
- WebSocket-based progress tracking
- Prometheus metrics
- Health checks and system status
- Comprehensive logging

### ğŸ”„ Data Management
- 3-hop data staging (Central â†’ Compute â†’ Worker â†’ Compute â†’ Central)
- Persistent caching with lineage tracking
- Automatic data integrity verification
- Support for multiple storage backends
- **Output Collection API**: List and download experiment outputs organized by task ID
- **Archive Generation**: Download all experiment outputs as a single tar.gz archive
- **Individual File Access**: Download specific output files with checksum verification

### ğŸ–¥ï¸ Command Line Interface
- **Complete CLI**: Full-featured command-line interface for all system operations
- **Data Management**: Upload/download files and directories to/from any storage type
- **Experiment Lifecycle**: Run, monitor, cancel, pause, resume, and retry experiments
- **Project Management**: Create projects, manage team members, and organize experiments
- **Resource Management**: Register compute/storage resources with credential binding and verification
- **Real-time Monitoring**: Watch experiments with live status updates and logs
- **Credential Security**: Verification-based credential binding with access testing

### ğŸš€ Scalability
- Horizontal scaling with multiple workers
- Rate limiting and resource management
- Caching layer for improved performance
- Event-driven architecture

## ğŸ“š Documentation

- [Architecture Guide](docs/reference/architecture.md) - System design and patterns
- [CLI Reference](docs/reference/cli.md) - Complete command-line interface documentation
- [Development Guide](docs/guides/development.md) - Development workflow and best practices
- [Deployment Guide](docs/guides/deployment.md) - Production deployment instructions
- [API Documentation](docs/reference/api_openapi.yaml) - Complete API specification
- [Testing Guide](tests/README.md) - Comprehensive testing documentation
- [Dashboard Integration](docs/guides/dashboard-integration.md) - Frontend integration guide
- [WebSocket Protocol](docs/reference/websocket-protocol.md) - Real-time communication protocol

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes following the hexagonal architecture
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## âš ï¸ Known Issues & Troubleshooting

### Cold Start Testing

When running cold-start tests from a fresh clone, ensure:

1. **SLURM Munge Key**: Run `./scripts/generate-slurm-munge-key.sh` to generate the shared authentication key
2. **Kubernetes Cluster**: The kind-cluster service may take 2-3 minutes to fully initialize
3. **Service Dependencies**: All services have proper health checks and startup dependencies

### Service Health Checks

All services now include health checks:
- **Scheduler**: HTTP health check on `/api/v2/health`
- **SLURM**: Munge authentication with shared key
- **Kubernetes**: Node readiness check
- **Storage**: Connection and availability checks

### Common Issues

- **SLURM Authentication Failures**: Regenerate munge key if nodes can't register
- **Kubernetes Cluster**: Kind cluster initialization is complex and may require manual setup
- **Port Conflicts**: Ensure ports 8080, 50051-50053, 5432, 8200, 9000-9001, 2222, 2049 are available

## ğŸ† Production Ready

This system is designed for production deployment with:
- âœ… Clean hexagonal architecture
- âœ… Comprehensive error handling
- âœ… Security best practices
- âœ… Monitoring and observability
- âœ… Scalability and performance
- âœ… Single authoritative database schema
- âœ… Event-driven real-time updates
- âœ… Complete API documentation