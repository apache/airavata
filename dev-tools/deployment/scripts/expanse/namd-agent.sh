#!/bin/bash -x

# #####################################################################
# NAMD Driver  + Airavata Agent for Expanse
# #####################################################################
#
# ----------------------------------------------------------------------
# CONTRIBUTORS
# ----------------------------------------------------------------------
# * Sudhakar Pamidigantham
# * Diego Gomes
# * Lahiru Jayathilake
# * Yasith Jayawardana
#
# ----------------------------------------------------------------------
# CHANGELOG
# ----------------------------------------------------------------------
# * 2024/12/13 - Agent subprocess and graceful shutdown (Yasith J)
# * 2024/12/09 - Reviewed (Diego Gomes)
# ######################################################################

########################################################################
# Part 1 - Housekeeping
########################################################################

#-----------------------------------------------------------------------
# Step 1.1 - Check command line
#-----------------------------------------------------------------------
if [ $# -lt 1 -o $# -gt 15 ]; then
  echo 1>&2 "Usage: $0 -t [CPU/GPU] -r [PJobID] -l [Continue_Replicas_list] -n [Number_of_Replicas] -i input_conf [SEAGrid_UserName] "
  exit 127
fi

# subdir depends on whether we're doing freq, water or PES. For freq and water,
# it should be hardcoded in the Xbaya workflow. For PES, it should be an
# additional array generated by the frontend. The contents of this array are
# trivial, but creating an extra Xbaya service to generate it would add
# unnecessary extra complexity. Besides, the frontend cannot avoid having to
# pass at least one array: the array with gjf files.

subdir="$PWD"
while getopts t:r:l:n:i:a:s: option; do
  case $option in
  t) ExeTyp=$OPTARG ;;
  r) PJobID=$OPTARG ;;
  l) rep_list=$OPTARG ;;
  n) num_rep=$OPTARG ;;
  i) input=$OPTARG ;;
  a) agent_id=$OPTARG ;;
  s) server_url=$OPTARG ;;
  \?) cat <<ENDCAT ;;
>! Usage: $0  [-et execution type cpu/gpu ]    !<
>!            [-rr  Previous JobID for continuation (optional)]      !<
>!            [-rl  replica list for contiuation (optional)]     !<
>!            [-rep Number of replicas  to run  (optional)]     !<
ENDCAT
  esac
done

echo "ExeTyp=$ExeTyp"
echo "PJobID=$PJobID"
echo "rep_list=$rep_list"
echo "num_rep=$num_rep"
echo "input=$input"
echo "agent_id=$agent_id"
echo "server_url=$server_url"

# ----------------------------------------------------------------------
# RUN AGENT AS SUBPROCESS (for now)
# ----------------------------------------------------------------------
SIF_PATH=/home/scigap/agent-framework/airavata-agent.sif
module load singularitypro
singularity exec --bind $PWD:/data $SIF_PATH bash -c "/opt/airavata-agent $server_url:19900 $agent_id" &
agent_pid=$! # save agent PID for graceful shutdown

#-----------------------------------------------------------------------
# Step 1.2 - Validate inputs
#-----------------------------------------------------------------------

if [ ! $AIRAVATA_USERNAME ]; then
  echo "Missing AIRAVATA_USERNAME. Check with support!"
  exit
fi

if [ ! $ExeTyp ]; then
  echo "Missing Execution Type: [CPU, GPU]"
  exit
fi

SG_UserName="$AIRAVATA_USERNAME"
echo "Execution Type: $ExeTyp"

#-----------------------------------------------------------------------
# Step 1.3 - Get the input configuration filename
#-----------------------------------------------------------------------
filename=$(basename -- "$input")
filename="${filename%.*}"

#-----------------------------------------------------------------------
# Step 1.4 - Copy previous files if this a continuation.
#-----------------------------------------------------------------------
if [ "$PJobID" ]; then
  cp $input saveInput #save configuration
  ls -lt $localarc/$PJobID/
  cp -r $localarc/$PJobID/. .
  cp saveInput $input
fi

#-----------------------------------------------------------------------
# Step 1.5 - Create folders for replicas (if necessary)
#-----------------------------------------------------------------------
echo " Creating folders for replica run(s)"
input_files=$(ls *.* | grep -v slurm)

# Create one subdirectory per replica and copy over the inputs
for i in $(seq 1 ${num_rep}); do
  if [ ! -d ${i} ]; then
    mkdir ${i}
    cp $input_files ${i}/
  fi
done

########################################################################
# Part 2 - Machine specific Options (SDSC-Expanse)
########################################################################

#-----------------------------------------------------------------------
# Step 2.1 - Load modules (SDSC-Expanse)
#-----------------------------------------------------------------------

module purge
module load slurm/expanse/current

if [ $ExeTyp = "CPU" ]; then
  echo "Loading CPU modules"
  module load cpu/0.17.3b gcc/10.2.0 openmpi/4.1.1
fi
if [ $ExeTyp = "GPU" ]; then
  echo "Loading GPU modules"
  module load gpu/0.17.3b
fi

module list

#-----------------------------------------------------------------------
# Step 2.2 - Set NAMD binary and command line for SDSC-Expanse
#-----------------------------------------------------------------------
APP_PATH=/home/scigap/applications
if [ $ExeTyp == "CPU" ]; then
  export NAMDPATH="$APP_PATH/NAMD_3.1alpha2_Linux-x86_64-multicore"
fi
if [ $ExeTyp == "GPU" ]; then
  export NAMDPATH="$APP_PATH/NAMD_3.1alpha2_Linux-x86_64-multicore-CUDA"
fi

#-----------------------------------------------------------------------
# Step 2.3 A - Run NAMD3 (CPU, Serial)
#-----------------------------------------------------------------------
# - one replica at a given time
# - each replica uses all CPUs
#-----------------------------------------------------------------------
if [ ${ExeTyp} == "CPU" ]; then
  for replica in $(seq 1 ${num_rep}); do
    cd ${subdir}/${replica}/ # Go to folder

    # Run NAMD3
    ${NAMDPATH}/namd3 \
      +setcpuaffinity \
      +p ${SLURM_CPUS_ON_NODE} \
      $input >${filename}.out 2>${filename}.err
  done
fi

#-----------------------------------------------------------------------
# Step 2.3 B - Run NAMD3 (GPU, Batched)
#-----------------------------------------------------------------------
# - one replica PER GPU at a given time
# - each replica uses all CPUs
#-----------------------------------------------------------------------
if [ ${ExeTyp} == "GPU" ]; then
  GPU_ID=0
  subtask_pids=()
  for replica in $(seq 1 ${num_rep}); do
    cd ${subdir}/${replica}/ # Go to folder

    # Run NAMD3 in background
    ${NAMDPATH}/namd3 \
      +setcpuaffinity \
      +p ${SLURM_CPUS_ON_NODE} \
      +devices ${GPU_ID} \
      $input >${filename}.out 2>${filename}.err &

    subtask_pids+=($!) # Store PID of the background NAMD task
    let GPU_ID+=1      # Increment GPU_ID
    
    # Wait for a batch of replicas to complete
    if [ ${GPU_ID} == ${SLURM_GPUS_ON_NODE} ]; then
      wait "${subtask_pids[@]}" # wait for current batch to complete
      subtask_pids=()           # clear subtask_pids of current batch
      GPU_ID=0                  # reset gpu counter
    fi
  done
  
  # Wait for the last batch of replicas to complete
  if [ ${#subtask_pids[@]} -gt 0 ]; then
    wait "${subtask_pids[@]}"   # wait for last batch to complete
    subtask_pids=()             # clear subtask_pids of last batch
  fi
fi

# Once done, go back to main folder
cd ${subdir}

########################################################################
# Part 3 - Output Flattening
########################################################################
num_rep=3
for replica in $(seq 1 ${num_rep}); do
  for file in $(ls ${replica}/*.*); do
    mv ${file} ${replica}"_"$(basename $file)
  done
  rm -rf ${replica}/
done

# Send SIGTERM to agent, and wait for completion
kill -TERM $agent_pid
wait $agent_pid

# Give it a break when jobs are done
sleep 10

# bye!
